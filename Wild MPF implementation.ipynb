{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from mpf_utils import load_data\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initalizeParameters(units = 32):\n",
    "    \"\"\"\n",
    "    Initialize an initial symmetric W matrix from a random normal distribution with mean 0 and variance 1/n.\n",
    "    Inputs:\n",
    "    - n: size of the Boltzmann Machine (BM).\n",
    "    \"\"\"\n",
    "    U = np.random.normal(loc = 0, scale = 1/units, size = (units, units))\n",
    "    return 0.5 * (U + U.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem encountered here: the Kcost function to compute the function is done in numpy, but if we are going to use Theano to do stochastic gradient descent, we have to use the tensors. But using tensors, we are not able to get the shape of our samples which complicate things. An attempt to manually give the size of samples and number of units lead to values of the bias very close to 0, which for now I have no idea why this is happening although my gradient checking shows that everything should be correct.\n",
    "\n",
    "Things to resolve:\n",
    "- how to use Theano to do stochastic gradient descent with my own gradient\n",
    "- why is the bias value learnt so close to 0, could it be some error lying somewhere that contribute to this deviation towards 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Kcost(x, W, temperature = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the cost computed by using the diagonals as the bias.\n",
    "    Inputs:\n",
    "    - x: samples used to train W.\n",
    "    - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "    - n: number of neurons in the BM.\n",
    "    - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "    \"\"\"\n",
    "    num_samples = 50000 #x.shape[0]        \n",
    "    num_units = 16 #x.shape[1]\n",
    "    delta = 1/2 - x\n",
    "    diag = np.diag(W)[:, None].T\n",
    "    E = delta * np.dot(x, W) - .25 * diag\n",
    "\n",
    "    cost = np.sum(np.exp(1/temperature * E)) / num_samples         \n",
    "    k = np.exp(E)        \n",
    "    D = np.dot((delta * k).T, x)         \n",
    "    C = np.zeros((num_units,))         \n",
    "    np.copyto(C, np.diag(D))                 \n",
    "    np.fill_diagonal(D, 0)         \n",
    "    C = C - .25 * np.sum(k, axis = 0)         \n",
    "    D = D + D.T         \n",
    "    np.fill_diagonal(D, C) \n",
    "\n",
    "    return cost, D/ num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mpf(object):\n",
    "    \"\"\"\n",
    "    Minimum probability flow with bias in the diagonals\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input = None, n = 32, temperature = 1,  W = None):\n",
    "        \n",
    "        self.n = n\n",
    "        self.temperature = temperature\n",
    "        U = np.random.rand(self.n, self.n)\n",
    "        R = 0.5 * (U + U.T)\n",
    "\n",
    "\n",
    "        if not W:\n",
    "            initial_W = np.asarray(R, dtype = theano.config.floatX)\n",
    "            W = theano.shared(initial_W, name = 'W', borrow = True)\n",
    "\n",
    "        self.W = W\n",
    "        \n",
    "        if input is None:\n",
    "            self.x = T.dmatrix(name = 'input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W]\n",
    "        \n",
    "    def Kcost(self, lr = 1e-2):\n",
    "        \"\"\"\n",
    "        Returns the cost computed by using the diagonals as the bias.\n",
    "        Inputs:\n",
    "        - x: samples used to train W.\n",
    "        - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "        - n: number of neurons in the BM.\n",
    "        - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "        \"\"\"\n",
    "        print (51 * '=')\n",
    "        print (24 * '#' + 'MPF' + 24 * '#')\n",
    "        print (51 * '=')\n",
    "        print (str(datetime.now()))\n",
    "        print ('Input size: {0}'.format(self.n))\n",
    "        print ('Learning temperature: {0}'.format(self.temperature))\n",
    "        print ('Learning rate: {0}'.format(lr))\n",
    "\n",
    "        num_samples = self.x.shape[0]        \n",
    "        num_units = self.x.shape[1]\n",
    "        delta = 1/2 - self.x\n",
    "        diag = T.diag(self.W)[:, None].T\n",
    "        E = delta * T.dot(self.x, self.W) - .25 * diag\n",
    "        \n",
    "        cost = T.sum(T.exp(1/self.temperature * E)) / num_samples         \n",
    "        k = T.exp(E)        \n",
    "        D = T.dot((delta * k).T, self.x)  \n",
    "        C = T.diag(D)\n",
    "        C = T.nlinalg.AllocDiag()(C)\n",
    "#         C = T.zeros((num_units,))         \n",
    "#         T.copyto(C, T.diag(D))                 \n",
    "        D = T.fill_diagonal(D, 0)         \n",
    "        C = C - .25 * T.sum(k, axis = 0)         \n",
    "        D = D + D.T + C         \n",
    "#         T.fill_diagonal(D, C) \n",
    "        \n",
    "        Wupdate = self.W - lr * (D/num_samples)\n",
    "        \n",
    "        updates = [(self.W, Wupdate)]\n",
    "\n",
    "        return cost, updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainmpf(units = 16, lr = 1e-2, n_epochs = 1000,\n",
    "             batchsize = 16, temperature = 1, validate_every = 100, sample = '16-50K.npy'):\n",
    "    \"\"\"\n",
    "    Trains parameters using MPF.\n",
    "    \"\"\"\n",
    "    \n",
    "    index = T.lscalar()\n",
    "    x = T.matrix('x')\n",
    "    flow = mpf(input = x, n = units, temperature = temperature)\n",
    "    \n",
    "    cost, updates = flow.Kcost(lr = lr)\n",
    "    \n",
    "    dataset = load_data(sample)\n",
    "    n_dataset_batches = dataset.get_value(borrow = True).shape[0] // batchsize\n",
    "\n",
    "    print ('Sample used: {0}'.format(sample))\n",
    "    print ('=' * 51)\n",
    "    \n",
    "    mpf_cost = theano.function(inputs = [index], outputs = cost, updates = updates, \\\n",
    "                                givens = {x: dataset[index * batchsize: (index + 1) * batchsize]})\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    best_mse = np.inf\n",
    "    best_W = [None, np.inf]\n",
    "    best_b = [None, np.inf]\n",
    "    best_epoch = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        c = []\n",
    "        current_time = timeit.default_timer()\n",
    "        for batch_index in range(n_dataset_batches):\n",
    "            c.append(mpf_cost(batch_index))\n",
    "        \n",
    "        W_learnt = flow.W.get_value(borrow = True)\n",
    "        b_learnt = np.zeros((units,))\n",
    "        np.copyto(b_learnt, np.diag(W_learnt))                        \n",
    "        np.fill_diagonal(W_learnt, 0)\n",
    "        \n",
    "        W = np.load(sample[0:2] + '-' + 'W' + '.npy')\n",
    "        b = np.load(sample[0:2] + '-' + 'b' + '.npy')\n",
    "                            \n",
    "        mseW = np.linalg.norm(W - (W_learnt))/ (units**2)\n",
    "        mseb = np.linalg.norm(b - b_learnt)/ units\n",
    "        mse = mseW + mseb\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_W[0] = W_learnt  \n",
    "            best_W[1] = mseW\n",
    "            best_b[0] = b_learnt  \n",
    "            best_b[1] = mseb\n",
    "            best_cost = np.mean(c, dtype='float64')\n",
    "            best_epoch = epoch\n",
    "        \n",
    "        if epoch%validate_every == 0:\n",
    "            print (b_learnt)\n",
    "            print ('Training epoch %d/%d, Cost: %f mseW: %.5f, mseb: %.5f, mse: %.5f, Time Elasped: %.2f '\\\n",
    "                 % (epoch, n_epochs, np.mean(c, dtype='float64'), \\\n",
    "                 mseW, mseb, mse,  (current_time - start_time)/60) )\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = end_time - start_time\n",
    "    fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize=(20,10))\n",
    "#     fig, ax = plt.subplots(figsize=(20,10))\n",
    "    fig.tight_layout()\n",
    "    plt.setp(ax, xticks=np.arange(0,100,16))\n",
    "\n",
    "#     plt.xticks(np.arange(0,100,16))\n",
    "    \n",
    "#     ax.plot(W.reshape(-1,1)[0:100], 'r')\n",
    "#     ax.plot((-W_learnt).reshape(-1,1)[0:100], 'b')\n",
    "#     ax.plot(-best_W[0].reshape(-1,1)[0:100], 'g')\n",
    "#     ax.set_title('W')\n",
    "#     ax.legend(['W', 'Learnt W','Best W'])\n",
    "    ax[0,0].plot(W.reshape(-1,1)[0:100], 'r')\n",
    "    ax[0,0].plot(W_learnt.reshape(-1,1)[0:100], 'b')\n",
    "    ax[0,0].plot(best_W[0].reshape(-1,1)[0:100], 'g')\n",
    "    ax[0,0].set_title('W')\n",
    "    ax[0,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[0,1].plot(W.reshape(-1,1)[101:200], 'r')\n",
    "    ax[0,1].plot(W_learnt.reshape(-1,1)[101:200], 'b')\n",
    "    ax[0,1].plot(best_W[0].reshape(-1,1)[101:200], 'g')\n",
    "    ax[0,1].set_title('W')\n",
    "    ax[0,1].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,0].plot(W.reshape(-1,1)[201:256], 'r')\n",
    "    ax[1,0].plot(W_learnt.reshape(-1,1)[201:256], 'b')\n",
    "    ax[1,0].plot(best_W[0].reshape(-1,1)[201:256], 'g')\n",
    "    ax[1,0].set_title('W')\n",
    "    ax[1,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,1].plot(b.reshape(-1,1), 'r')\n",
    "    ax[1,1].plot(b_learnt.reshape(-1,1),'b')\n",
    "    ax[1,1].plot(best_b[0].reshape(-1,1),'g')\n",
    "    ax[1,1].set_title('b')\n",
    "    ax[1,1].legend(['b', 'Learnt b','Best b'])\n",
    "\n",
    "    \n",
    "    print ('The training took %.2f minutes' % (training_time/60.))\n",
    "    print ('#' * 22 + 'Results' + '#' * 22)\n",
    "    print ('=' * 51)\n",
    "    print ('Best mse: {0}'.format(best_mse))\n",
    "    print ('Best W mse: {0}'.format(best_W[1]))\n",
    "    print ('Best b mse: {0}'.format(best_b[1]))\n",
    "    print ('Best epoch: {0}'.format(best_epoch))\n",
    "    print ('=' * 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "########################MPF########################\n",
      "===================================================\n",
      "2017-06-01 20:40:16.081546\n",
      "Input size: 16\n",
      "Learning temperature: 1\n",
      "Learning rate: 0.01\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TensorVariable' object has no attribute 'get_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5022d2dde21f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainmpf(units = 16, lr = 1e-2, n_epochs = 1000,\n\u001b[0;32m----> 2\u001b[0;31m              batchsize = 16, temperature = 1, validate_every = 100, sample = '16-50K.npy')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-26aa2eab0106>\u001b[0m in \u001b[0;36mtrainmpf\u001b[0;34m(units, lr, n_epochs, batchsize, temperature, validate_every, sample)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mflow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7989ab8079f4>\u001b[0m in \u001b[0;36mKcost\u001b[0;34m(self, lr)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Learning rate: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mnum_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorVariable' object has no attribute 'get_value'"
     ]
    }
   ],
   "source": [
    "trainmpf(units = 16, lr = 1e-2, n_epochs = 1000,\n",
    "             batchsize = 16, temperature = 1, validate_every = 100, sample = '16-50K.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.load('16-b.npy')\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
