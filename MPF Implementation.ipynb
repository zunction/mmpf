{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "\n",
    "\n",
    "import os\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from mpfntutils import load_data\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initalizeParameters(units = 32):\n",
    "    \"\"\"\n",
    "    Initialize an initial symmetric W matrix from a random normal distribution with mean 0 and variance 1/n.\n",
    "    Inputs:\n",
    "    - n: size of the Boltzmann Machine (BM).\n",
    "    \"\"\"\n",
    "    U = np.random.normal(loc = 0, scale = 1/units, size = (units, units))\n",
    "    return 0.5 * (U + U.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old cost and gradient    \n",
    "\n",
    "# def Kcost(x, W, temperature = 1, units = 32):\n",
    "#         \"\"\"\n",
    "#         Returns the cost computed by using the diagonals as the bias.\n",
    "#         Inputs:\n",
    "#         - x: samples used to train W.\n",
    "#         - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "#         - n: number of neurons in the BM.\n",
    "#         - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         cost = np.mean(np.exp(1/temperature * ((0.5 - x) * np.dot(x, W)) \\\n",
    "#                               - 0.25 * np.diag(W)))\n",
    "               \n",
    "#         D = np.dot((0.5 - x).T, x)\n",
    "#         Wgrad = cost * ((D + D.T) - (D * np.eye(units)) + 0.25 * np.eye(units))\n",
    "#         return cost  \n",
    "#         new one starts after here\n",
    "        \n",
    "def Kcost(x, W, temperature = 1):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the cost computed by using the diagonals as the bias.\n",
    "    Inputs:\n",
    "    - x: samples used to train W.\n",
    "    - W: weights between the neurons of the Boltzmann Machine (BM).\n",
    "    - n: number of neurons in the BM.\n",
    "    - temperature: keep it as 1 until cost grows too big then raise temperature.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]        \n",
    "    num_units = x.shape[1]\n",
    "    delta = 1/2 - x\n",
    "    diag = np.diag(W)[:, None].T\n",
    "    E = delta * np.dot(x, W) - .25 * diag\n",
    "\n",
    "    cost = np.sum(np.exp(1/temperature * E)) / num_samples         \n",
    "    k = np.exp(E)        \n",
    "    D = np.dot((delta * k).T, x)         \n",
    "    C = np.zeros((num_units,))         \n",
    "    np.copyto(C, np.diag(D))                 \n",
    "    np.fill_diagonal(D, 0)         \n",
    "    C = C - .25 * np.sum(k, axis = 0)         \n",
    "    D = D + D.T         \n",
    "    np.fill_diagonal(D, C) \n",
    "\n",
    "    return cost, D/ num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def params_adam_opt(grad, m = 0, v = 0, beta1 = 0.9, beta2 = 0.999):\n",
    "    \"\"\"\n",
    "    Adaptive Moment Estimation (Adam) optimizer. Takes in parameters beta1, beta2 for adaptive \n",
    "    optimization. Returns next iteration of m and v for computing the grad update.\n",
    "    \n",
    "    Reference: http://arxiv.org/abs/1412.6980\n",
    "    \"\"\"\n",
    "    m = beta1 * m + (1 - beta1) * grad\n",
    "    v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "    return m, v\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainmpf(units = 32, lr = 1e-2, eps = 1e-8, n_epochs = 1000,\n",
    "             batchsize = 32, temperature = 1, validate_every = 100, sample = '32-50K.npy'):\n",
    "    \"\"\"\n",
    "    Trains parameters using MPF without using symbolic gradient.\n",
    "    Inputs:\n",
    "    - units: number of neurons in the BM.\n",
    "    - lr: learning rate.\n",
    "    - eps: parameter for Adam optimizer\n",
    "    - n_epochs: number of epochs to train.\n",
    "    - batchsize: size of batches for stochastic gradient descent.\n",
    "    - temperature: happy to be 1 for now.\n",
    "    - sample: sample used.\n",
    "    \n",
    "    \"\"\"\n",
    "    print (51 * '=')\n",
    "    print (24 * '#' + 'MPF' + 24 * '#')\n",
    "    print (51 * '=')\n",
    "    print (str(datetime.now()))\n",
    "    print ('Input size: {0}'.format(units))\n",
    "    print ('Learning temperature: {0}'.format(temperature))\n",
    "    print ('Learning rate: {0}'.format(lr))\n",
    "    \n",
    "    dataset = load_data(sample)\n",
    "    n_dataset_batches = dataset.shape[0] // batchsize\n",
    "    W = initalizeParameters(units = units)\n",
    "    \n",
    "    print ('Sample used: {0}'.format(sample))\n",
    "    print ('=' * 51)\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "# #     Extracting learnt parameters\n",
    "#     W_learnt = np.zeros((units, units))\n",
    "#     b_learnt = np.zeros((units,))\n",
    "# #     Renaming to W_learnt and b_learnt\n",
    "#     np.copyto(b_learnt, np.diag(W))\n",
    "#     np.copyto(W_learnt, W)\n",
    "#     np.fill_diagonal(W_learnt, 0)\n",
    "\n",
    "#     Loading the original W and b\n",
    "    org_W = np.load(sample[0:2] + '-' + 'W' + '.npy')\n",
    "    org_b = np.load(sample[0:2] + '-' + 'b' + '.npy')\n",
    "   \n",
    "    \n",
    "#     Tracking best parameters learnt\n",
    "    best_mse = np.inf\n",
    "    best_W = [None, np.inf]\n",
    "    best_b = [None, np.inf]\n",
    "    best_epoch = np.inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        c = []\n",
    "        current_time = timeit.default_timer()\n",
    "        for batch_index in range(n_dataset_batches):\n",
    "            minibatch = dataset[batch_index * batchsize: (batch_index + 1) * batchsize]\n",
    "            cost, grad = Kcost(dataset, W)\n",
    "#             Extracting learnt parameters\n",
    "            W_learnt = np.zeros((units, units))\n",
    "            b_learnt = np.zeros((units,))\n",
    "#             Renaming to W_learnt and b_learnt\n",
    "            np.copyto(b_learnt, np.diag(W))\n",
    "            np.copyto(W_learnt, W)\n",
    "            np.fill_diagonal(W_learnt, 0)\n",
    "            \n",
    "#             m, v = params_adam_opt(grad)\n",
    "#             W += -lr * m / (np.sqrt(v) + eps)\n",
    "            W += -lr * grad\n",
    "            c.append(cost)\n",
    "            \n",
    "\n",
    "        mseW = np.linalg.norm(org_W - W_learnt)/ ((units**2 - units) / 2)\n",
    "        mseb = np.linalg.norm(org_b - b_learnt)/ units\n",
    "        mse = mseW + mseb\n",
    "\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_W[0] = W_learnt  \n",
    "            best_W[1] = mseW\n",
    "            best_b[0] = b_learnt  \n",
    "            best_b[1] = mseb\n",
    "            best_cost = np.mean(c, dtype='float64')\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if epoch%validate_every == 0:\n",
    "            print ('Training epoch %d/%d, Cost: %f mseW: %.5f, mseb: %.5f, mse: %.5f, Time Elasped: %.2f '\\\n",
    "                 % (epoch, n_epochs, np.mean(c, dtype='float64'), \\\n",
    "                 mseW, mseb, mse,  (current_time - start_time)/60) )\n",
    "    \n",
    "    end_time = timeit.default_timer()\n",
    "    \n",
    "    training_time = end_time - start_time\n",
    "    fig, ax = plt.subplots(nrows = 2, ncols = 2, figsize=(20,10))\n",
    "    fig.tight_layout()\n",
    "    plt.setp(ax, xticks=np.arange(0,100,16))\n",
    "    ax[0,0].plot(org_W.reshape(-1,1)[0:100], 'r')\n",
    "#     ax[0,0].plot(W_learnt.reshape(-1,1)[0:100], 'b')\n",
    "    ax[0,0].plot(best_W[0].reshape(-1,1)[0:100], 'g')\n",
    "    ax[0,0].set_title('W')\n",
    "    ax[0,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[0,1].plot(org_W.reshape(-1,1)[101:200], 'r')\n",
    "#     ax[0,1].plot(W_learnt.reshape(-1,1)[101:200], 'b')\n",
    "    ax[0,1].plot(best_W[0].reshape(-1,1)[101:200], 'g')\n",
    "    ax[0,1].set_title('W')\n",
    "    ax[0,1].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,0].plot(org_W.reshape(-1,1)[201:256], 'r')\n",
    "#     ax[1,0].plot(W_learnt.reshape(-1,1)[201:256], 'b')\n",
    "    ax[1,0].plot(best_W[0].reshape(-1,1)[201:256], 'g')\n",
    "    ax[1,0].set_title('W')\n",
    "    ax[1,0].legend(['W', 'Learnt W','Best W'])\n",
    "    ax[1,1].plot(org_b.reshape(-1,1), 'r')\n",
    "#     ax[1,1].plot(b_learnt.reshape(-1,1),'b')\n",
    "    ax[1,1].plot(best_b[0].reshape(-1,1),'g')\n",
    "    ax[1,1].set_title('b')\n",
    "    ax[1,1].legend(['b', 'Learnt b','Best b'])\n",
    "\n",
    "    \n",
    "    print ('The training took %.2f minutes' % (training_time/60.))\n",
    "    print ('#' * 22 + 'Results' + '#' * 22)\n",
    "    print ('=' * 51)\n",
    "    print ('Best mse: {0}'.format(best_mse))\n",
    "    print ('Best W mse: {0}'.format(best_W[1]))\n",
    "    print ('Best b mse: {0}'.format(best_b[1]))\n",
    "    print ('Best epoch: {0}'.format(best_epoch))\n",
    "    print ('=' * 51)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "########################MPF########################\n",
      "===================================================\n",
      "2017-06-01 08:53:16.608582\n",
      "Input size: 16\n",
      "Learning temperature: 1\n",
      "Learning rate: 0.01\n",
      "Sample used: 16-50K.npy\n",
      "===================================================\n",
      "Training epoch 0/100, Cost: 3.871938 mseW: 0.07188, mseb: 1.01059, mse: 1.08247, Time Elasped: 0.00 \n",
      "Training epoch 10/100, Cost: 0.203961 mseW: 0.08102, mseb: 2.09058, mse: 2.17161, Time Elasped: 21.70 \n",
      "Training epoch 20/100, Cost: 0.106927 mseW: 0.08762, mseb: 2.40877, mse: 2.49639, Time Elasped: 46.73 \n",
      "Training epoch 30/100, Cost: 0.072676 mseW: 0.09213, mseb: 2.60309, mse: 2.69521, Time Elasped: 69.78 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-357d839e5b16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainmpf(units = 16, lr = 1e-2, eps = 1e-8, n_epochs = 100,\n\u001b[0;32m----> 2\u001b[0;31m              batchsize = 16, temperature = 1, validate_every = 10, sample = '16-50K.npy')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-cea4eb21b837>\u001b[0m in \u001b[0;36mtrainmpf\u001b[0;34m(units, lr, eps, n_epochs, batchsize, temperature, validate_every, sample)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_dataset_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[1;31m#             Extracting learnt parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mW_learnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-728a2aacc177>\u001b[0m in \u001b[0;36mKcost\u001b[0;34m(x, W, temperature)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtemperature\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainmpf(units = 16, lr = 1e-2, eps = 1e-8, n_epochs = 100,\n",
    "             batchsize = 16, temperature = 1, validate_every = 10, sample = '16-50K.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
